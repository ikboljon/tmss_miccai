{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'grpc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ENG~1.ROB\\AppData\\Local\\Temp/ipykernel_23828/2352321777.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpydoc\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcli\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtraceback\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgrpc\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mClientCallDetails\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'grpc'"
     ]
    }
   ],
   "source": [
    "import imp\n",
    "from pydoc import cli\n",
    "from traceback import print_tb\n",
    "# from grpc import ClientCallDetails\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchmtlr import MTLR\n",
    "from torchmtlr.utils import make_time_bins, encode_survival\n",
    "import SimpleITK as sitk\n",
    "import nibabel as nib\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "from typing import Sequence, Tuple, Union, Optional, Callable, Any, List\n",
    "\n",
    "from monai.networks.blocks.dynunet_block import UnetOutBlock\n",
    "from monai.networks.blocks.unetr_block import UnetrBasicBlock, UnetrPrUpBlock, UnetrUpBlock\n",
    "#from monai.networks.nets.vit import ViT\n",
    "from monai.utils import ensure_tuple_rep\n",
    "from monai.config.type_definitions import NdarrayOrTensor, NdarrayTensor\n",
    "from monai.transforms.transform import Transform\n",
    "from monai.utils.enums import TransformBackends\n",
    "from monai.utils.module import look_up_option\n",
    "from monai.transforms.croppad.array import Pad\n",
    "from monai.utils import (\n",
    "    InterpolateMode,\n",
    "    NumpyPadMode,\n",
    "    PytorchPadMode,\n",
    "    ensure_tuple_rep\n",
    ")\n",
    "from monai.utils.type_conversion import convert_data_type, convert_to_dst_type\n",
    "from einops import repeat, rearrange\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import elasticdeform\n",
    "from skimage.transform import rotate\n",
    "\n",
    "\n",
    "\n",
    "random.seed(260520)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TRASNFORMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compose:\n",
    "    def __init__(self, transforms=None):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        for transform in self.transforms:\n",
    "            sample = transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "class ToTensor:\n",
    "    def __init__(self, mode='train'):\n",
    "        if mode not in ['train', 'test']:\n",
    "            raise ValueError(f\"Argument 'mode' must be 'train' or 'test'. Received {mode}\")\n",
    "        self.mode = mode\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        if self.mode == 'train':\n",
    "            img, mask = sample['input'], sample['target_mask']\n",
    "            img = np.transpose(img, axes=[3, 0, 1, 2])\n",
    "            mask = np.transpose(mask, axes=[3, 0, 1, 2])\n",
    "            img = torch.from_numpy(img).float()\n",
    "            mask = torch.from_numpy(mask).float()\n",
    "            sample['input'], sample['target_mask'] = img, mask\n",
    "\n",
    "        else:  # if self.mode == 'test'\n",
    "            img = sample['input']\n",
    "            img = np.transpose(img, axes=[3, 0, 1, 2])\n",
    "            img = torch.from_numpy(img).float()\n",
    "            sample['input'] = img\n",
    "\n",
    "        return sample        \n",
    "\n",
    "\n",
    "class Mirroring:\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "        random.seed(260520)\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        if random.random() < self.p:\n",
    "            img, mask = sample['input'], sample['target_mask']\n",
    "\n",
    "            n_axes = random.randint(0, 3)\n",
    "            random_axes = random.sample(range(3), n_axes)\n",
    "\n",
    "            img = np.flip(img, axis=tuple(random_axes))\n",
    "            mask = np.flip(mask, axis=tuple(random_axes))\n",
    "\n",
    "            sample['input'], sample['target_mask'] = img.copy(), mask.copy()\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "class NormalizeIntensity:\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        img = sample['input']\n",
    "        img[:, :, :, 0] = self.normalize_ct(img[:, :, :, 0])\n",
    "        img[:, :, :, 1] = self.normalize_pt(img[:, :, :, 1])\n",
    "\n",
    "        sample['input'] = img\n",
    "        return sample\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_ct(img):\n",
    "        norm_img = np.clip(img, -1024, 1024) / 1024\n",
    "        return norm_img\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_pt(img):\n",
    "        mean = np.mean(img)\n",
    "        std = np.std(img)\n",
    "        return (img - mean) / (std + 1e-3)\n",
    "\n",
    "\n",
    "class RandomRotation:\n",
    "\n",
    "    def __init__(self, p=0.5, angle_range=[5, 15]):\n",
    "        self.p = p\n",
    "        self.angle_range = angle_range\n",
    "        random.seed(260520)\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        if random.random() < self.p:\n",
    "            img, mask = sample['input'], sample['target_mask']\n",
    "\n",
    "            num_of_seqs = img.shape[-1]\n",
    "            n_axes = random.randint(1, 3)\n",
    "            random_axes = random.sample([0, 1, 2], n_axes)\n",
    "\n",
    "            for axis in random_axes:\n",
    "\n",
    "                angle = random.randrange(*self.angle_range)\n",
    "                angle = -angle if random.random() < 0.5 else angle\n",
    "\n",
    "                for i in range(num_of_seqs):\n",
    "                    img[:, :, :, i] = RandomRotation.rotate_3d_along_axis(img[:, :, :, i], angle, axis, 1)\n",
    "\n",
    "                mask[:, :, :, 0] = RandomRotation.rotate_3d_along_axis(mask[:, :, :, 0], angle, axis, 0)\n",
    "\n",
    "            sample['input'], sample['target_mask'] = img, mask\n",
    "        return sample\n",
    "\n",
    "    @staticmethod\n",
    "    def rotate_3d_along_axis(img, angle, axis, order):\n",
    "\n",
    "        if axis == 0:\n",
    "            rot_img = rotate(img, angle, order=order, preserve_range=True)\n",
    "\n",
    "        if axis == 1:\n",
    "            rot_img = np.transpose(img, axes=(1, 2, 0))\n",
    "            rot_img = rotate(rot_img, angle, order=order, preserve_range=True)\n",
    "            rot_img = np.transpose(rot_img, axes=(2, 0, 1))\n",
    "\n",
    "        if axis == 2:\n",
    "            rot_img = np.transpose(img, axes=(2, 0, 1))\n",
    "            rot_img = rotate(rot_img, angle, order=order, preserve_range=True)\n",
    "            rot_img = np.transpose(rot_img, axes=(1, 2, 0))\n",
    "\n",
    "        return rot_img\n",
    "\n",
    "\n",
    "class ZeroPadding:\n",
    "\n",
    "    def __init__(self, target_shape, mode='train'):\n",
    "        self.target_shape = np.array(target_shape)  # without channel dimension\n",
    "        if mode not in ['train', 'test']:\n",
    "            raise ValueError(f\"Argument 'mode' must be 'train' or 'test'. Received {mode}\")\n",
    "        self.mode = mode\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        if self.mode == 'train':\n",
    "            img, mask = sample['input'], sample['target_mask']\n",
    "\n",
    "            input_shape = np.array(img.shape[:-1])  # last (channel) dimension is ignored\n",
    "            d_x, d_y, d_z = self.target_shape - input_shape\n",
    "            d_x, d_y, d_z = int(d_x), int(d_y), int(d_z)\n",
    "\n",
    "            if not all(i == 0 for i in (d_x, d_y, d_z)):\n",
    "                positive = [i if i > 0 else 0 for i in (d_x, d_y, d_z)]\n",
    "                negative = [i if i < 0 else None for i in (d_x, d_y, d_z)]\n",
    "\n",
    "                # padding for positive values:\n",
    "                img = np.pad(img, ((0, positive[0]), (0, positive[1]), (0, positive[2]), (0, 0)), 'constant', constant_values=(0, 0))\n",
    "                mask = np.pad(mask, ((0, positive[0]), (0, positive[1]), (0, positive[2]), (0, 0)), 'constant', constant_values=(0, 0))\n",
    "\n",
    "                # cropping for negative values:\n",
    "                img = img[: negative[0], : negative[1], : negative[2], :].copy()\n",
    "                mask = mask[: negative[0], : negative[1], : negative[2], :].copy()\n",
    "\n",
    "                assert img.shape[:-1] == mask.shape[:-1], f'Shape mismatch for the image {img.shape[:-1]} and mask {mask.shape[:-1]}'\n",
    "\n",
    "                sample['input'], sample['target_mask'] = img, mask\n",
    "\n",
    "            return sample\n",
    "\n",
    "        else:  # if self.mode == 'test'\n",
    "            img = sample['input']\n",
    "\n",
    "            input_shape = np.array(img.shape[:-1])  # last (channel) dimension is ignored\n",
    "            d_x, d_y, d_z = self.target_shape - input_shape\n",
    "            d_x, d_y, d_z = int(d_x), int(d_y), int(d_z)\n",
    "\n",
    "            if not all(i == 0 for i in (d_x, d_y, d_z)):\n",
    "                positive = [i if i > 0 else 0 for i in (d_x, d_y, d_z)]\n",
    "                negative = [i if i < 0 else None for i in (d_x, d_y, d_z)]\n",
    "\n",
    "                # padding for positive values:\n",
    "                img = np.pad(img, ((0, positive[0]), (0, positive[1]), (0, positive[2]), (0, 0)), 'constant', constant_values=(0, 0))\n",
    "\n",
    "                # cropping for negative values:\n",
    "                img = img[: negative[0], : negative[1], : negative[2], :].copy()\n",
    "\n",
    "                sample['input'] = img\n",
    "\n",
    "            return sample\n",
    "\n",
    "\n",
    "class ExtractPatch:\n",
    "    \"\"\"Extracts a patch of a given size from an image (4D numpy array).\"\"\"\n",
    "\n",
    "    def __init__(self, patch_size, p_tumor=0.5):\n",
    "        self.patch_size = patch_size  # without channel dimension!\n",
    "        self.p_tumor = p_tumor  # probs to extract a patch with a tumor\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        img = sample['input']\n",
    "        mask = sample['target_mask']\n",
    "\n",
    "        assert all(x <= y for x, y in zip(self.patch_size, img.shape[:-1])), \\\n",
    "            f\"Cannot extract the patch with the shape {self.patch_size} from  \" \\\n",
    "                f\"the image with the shape {img.shape}.\"\n",
    "\n",
    "        # patch_size components:\n",
    "        ps_x, ps_y, ps_z = self.patch_size\n",
    "\n",
    "        if random.random() < self.p_tumor:\n",
    "            # coordinates of the tumor's center:\n",
    "            xs, ys, zs, _ = np.where(mask != 0)\n",
    "            tumor_center_x = np.min(xs) + (np.max(xs) - np.min(xs)) // 2\n",
    "            tumor_center_y = np.min(ys) + (np.max(ys) - np.min(ys)) // 2\n",
    "            tumor_center_z = np.min(zs) + (np.max(zs) - np.min(zs)) // 2\n",
    "\n",
    "            # compute the origin of the patch:\n",
    "            patch_org_x = random.randint(tumor_center_x - ps_x, tumor_center_x)\n",
    "            patch_org_x = np.clip(patch_org_x, 0, img.shape[0] - ps_x)\n",
    "\n",
    "            patch_org_y = random.randint(tumor_center_y - ps_y, tumor_center_y)\n",
    "            patch_org_y = np.clip(patch_org_y, 0, img.shape[1] - ps_y)\n",
    "\n",
    "            patch_org_z = random.randint(tumor_center_z - ps_z, tumor_center_z)\n",
    "            patch_org_z = np.clip(patch_org_z, 0, img.shape[2] - ps_z)\n",
    "        else:\n",
    "            patch_org_x = random.randint(0, img.shape[0] - ps_x)\n",
    "            patch_org_y = random.randint(0, img.shape[1] - ps_y)\n",
    "            patch_org_z = random.randint(0, img.shape[2] - ps_z)\n",
    "\n",
    "        # extract the patch:\n",
    "        patch_img = img[patch_org_x: patch_org_x + ps_x,\n",
    "                    patch_org_y: patch_org_y + ps_y,\n",
    "                    patch_org_z: patch_org_z + ps_z,\n",
    "                    :].copy()\n",
    "\n",
    "        patch_mask = mask[patch_org_x: patch_org_x + ps_x,\n",
    "                     patch_org_y: patch_org_y + ps_y,\n",
    "                     patch_org_z: patch_org_z + ps_z,\n",
    "                     :].copy()\n",
    "\n",
    "        assert patch_img.shape[:-1] == self.patch_size, \\\n",
    "            f\"Shape mismatch for the patch with the shape {patch_img.shape[:-1]}, \" \\\n",
    "                f\"whereas the required shape is {self.patch_size}.\"\n",
    "\n",
    "        sample['input'] = patch_img\n",
    "        sample['target_mask'] = patch_mask\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "class InverseToTensor:\n",
    "    def __call__(self, sample):\n",
    "        output = sample['output']\n",
    "\n",
    "        output = torch.squeeze(output)  # squeeze the batch and channel dimensions\n",
    "        output = output.numpy()\n",
    "\n",
    "        sample['output'] = output\n",
    "        return sample\n",
    "\n",
    "\n",
    "class CheckOutputShape:\n",
    "    def __init__(self, shape=(144, 144, 144)):\n",
    "        self.shape = shape\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        output = sample['output']\n",
    "        assert output.shape == self.shape, \\\n",
    "            f'Received wrong output shape. Must be {self.shape}, but received {output.shape}.'\n",
    "        return sample\n",
    "\n",
    "\n",
    "class ProbsToLabels:\n",
    "    def __call__(self, sample):\n",
    "        output = sample['output']\n",
    "        output = (output > 0.5).astype(int)  # get binary label\n",
    "        sample['output'] = output\n",
    "        return sample\n",
    "\n",
    "class AdjustContrast(Transform):\n",
    "    \"\"\"\n",
    "    Changes image intensity by gamma. Each pixel/voxel intensity is updated as::\n",
    "        x = ((x - min) / intensity_range) ^ gamma * intensity_range + min\n",
    "    Args:\n",
    "        gamma: gamma value to adjust the contrast as function.\n",
    "    \"\"\"\n",
    "\n",
    "    backend = [TransformBackends.TORCH, TransformBackends.NUMPY]\n",
    "\n",
    "    def __init__(self, gamma: float, random=True) -> None:\n",
    "        if not isinstance(gamma, (int, float)):\n",
    "            raise ValueError(\"gamma must be a float or int number.\")\n",
    "        self.gamma = gamma\n",
    "        self.random = random\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        \"\"\"\n",
    "        Apply the transform to `img`.\n",
    "        \"\"\"\n",
    "        if self.random:\n",
    "            self.gamma = np.random.uniform(0.5, 2.0)\n",
    "\n",
    "        images, mask = sample['input'], sample['target_mask']\n",
    "        ct_img = images[:,:,:,0]\n",
    "        pet_img = images[:,:,:,1]\n",
    "        \n",
    "        \n",
    "        epsilon = 1e-7\n",
    "        img_min = pet_img.min()\n",
    "        img_range = pet_img.max() - img_min\n",
    "        \n",
    "        ret: NdarrayOrTensor = ((pet_img - img_min) / float(img_range + epsilon)) ** self.gamma * img_range + img_min\n",
    "        img = np.stack([ct_img, ret], axis=-1)\n",
    "\n",
    "        sample['input'] = img\n",
    "\n",
    "        return sample\n",
    "class AdjustContrastCT(Transform):\n",
    "    \"\"\"\n",
    "    Changes image intensity by gamma. Each pixel/voxel intensity is updated as::\n",
    "        x = ((x - min) / intensity_range) ^ gamma * intensity_range + min\n",
    "    Args:\n",
    "        gamma: gamma value to adjust the contrast as function.\n",
    "    \"\"\"\n",
    "\n",
    "    backend = [TransformBackends.TORCH, TransformBackends.NUMPY]\n",
    "\n",
    "    def __init__(self, gamma: float, p=0.5, random=True) -> None:\n",
    "        if not isinstance(gamma, (int, float)):\n",
    "            raise ValueError(\"gamma must be a float or int number.\")\n",
    "        self.p = p\n",
    "        self.gamma = gamma\n",
    "        self.random = random\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        \"\"\"\n",
    "        Apply the transform to `img`.\n",
    "        \"\"\"\n",
    "        if random.random() <self.p:\n",
    "            if self.random:\n",
    "                self.gamma = np.random.uniform(0.5, 2.0)\n",
    "\n",
    "            images, mask = sample['input'], sample['target_mask']\n",
    "            ct_img = images[:,:,:,0]\n",
    "            pet_img = images[:,:,:,1]\n",
    "            \n",
    "            \n",
    "            epsilon = 1e-7\n",
    "            img_min = ct_img.min()\n",
    "            img_range = ct_img.max() - img_min\n",
    "            \n",
    "            ret: NdarrayOrTensor = ((ct_img - img_min) / float(img_range + epsilon)) ** self.gamma * img_range + img_min\n",
    "            img = np.stack([ret, pet_img], axis=-1)\n",
    "\n",
    "            sample['input'] = img\n",
    "\n",
    "        return sample\n",
    "\n",
    "class Zoom(Transform):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "    def __call__(self, sample):\n",
    "        images, mask = sample['input'], sample['target_mask']\n",
    "        ct_img = images[:,:,:,0]\n",
    "        pet = images[:,:,:,1]\n",
    "        mask = mask.squeeze(-1)\n",
    "        \n",
    "        zoomed_ct = zoom(ct_img, self.factor)\n",
    "        zoomed_pet = zoom(pet, self.factor)\n",
    "        \n",
    "        img = np.stack([zoomed_ct, zoomed_pet], axis=-1)\n",
    "        sample['input'] = img\n",
    "\n",
    "\n",
    "        zoomed_mask = zoom(mask, self.factor)\n",
    "        zoomed_mask[zoomed_mask<0.5] = 0\n",
    "        zoomed_mask[zoomed_mask>0] = 1  \n",
    "        sample['target_mask'] = np.expand_dims(zoomed_mask, axis=-1)\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "def zoom(\n",
    "    img,\n",
    "    factor,\n",
    "    padding_mode: Optional[Union[NumpyPadMode, PytorchPadMode, str]] = None,\n",
    "    align_corners: Optional[bool] = True,\n",
    "    keep_size = True,\n",
    "\n",
    ") -> NdarrayOrTensor:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        img: channel first array, must have shape: (num_channels, H[, W, ..., ]).\n",
    "        mode: {``\"nearest\"``, ``\"linear\"``, ``\"bilinear\"``, ``\"bicubic\"``, ``\"trilinear\"``, ``\"area\"``}\n",
    "            The interpolation mode. Defaults to ``self.mode``.\n",
    "            See also: https://pytorch.org/docs/stable/nn.functional.html#interpolate\n",
    "        padding_mode: available modes for numpy array:{``\"constant\"``, ``\"edge\"``, ``\"linear_ramp\"``, ``\"maximum\"``,\n",
    "            ``\"mean\"``, ``\"median\"``, ``\"minimum\"``, ``\"reflect\"``, ``\"symmetric\"``, ``\"wrap\"``, ``\"empty\"``}\n",
    "            available modes for PyTorch Tensor: {``\"constant\"``, ``\"reflect\"``, ``\"replicate\"``, ``\"circular\"``}.\n",
    "            One of the listed string values or a user supplied function. Defaults to ``\"constant\"``.\n",
    "            The mode to pad data after zooming.\n",
    "            See also: https://numpy.org/doc/1.18/reference/generated/numpy.pad.html\n",
    "            https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html\n",
    "        align_corners: This only has an effect when mode is\n",
    "            'linear', 'bilinear', 'bicubic' or 'trilinear'. Defaults to ``self.align_corners``.\n",
    "            See also: https://pytorch.org/docs/stable/nn.functional.html#interpolate\n",
    "    \"\"\"\n",
    "    \n",
    "    img_t: torch.Tensor\n",
    "    img_t, *_ = convert_data_type(img, torch.Tensor, dtype=torch.float32)  # type: ignore\n",
    "    mode = InterpolateMode('bilinear')\n",
    "    _zoom = ensure_tuple_rep(factor, img.ndim - 1)  # match the spatial image dim\n",
    "    zoomed: NdarrayOrTensor = torch.nn.functional.interpolate(  # type: ignore\n",
    "        recompute_scale_factor=True,\n",
    "        input=img_t.unsqueeze(0),\n",
    "        scale_factor=list(_zoom),\n",
    "        mode=look_up_option(mode if mode is None else mode, InterpolateMode).value,\n",
    "        align_corners=align_corners if align_corners is None else align_corners,\n",
    "    )\n",
    "    zoomed = zoomed.squeeze(0)\n",
    "\n",
    "    if keep_size and not np.allclose(img_t.shape, zoomed.shape):\n",
    "\n",
    "        pad_vec = [(0, 0)] * len(img_t.shape)\n",
    "        slice_vec = [slice(None)] * len(img_t.shape)\n",
    "        for idx, (od, zd) in enumerate(zip(img_t.shape, zoomed.shape)):\n",
    "            diff = od - zd\n",
    "            half = abs(diff) // 2\n",
    "            if diff > 0:  # need padding\n",
    "                pad_vec[idx] = (half, diff - half)\n",
    "            elif diff < 0:  # need slicing\n",
    "                slice_vec[idx] = slice(half, half + od)\n",
    "\n",
    "        padder = Pad(pad_vec, padding_mode or padding_mode)\n",
    "        zoomed = padder(zoomed)\n",
    "        zoomed = zoomed[tuple(slice_vec)]\n",
    "\n",
    "    out, *_ = convert_to_dst_type(zoomed, dst=img)\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "class ElasticDeformation():\n",
    "    def __init__(self, p = 0.5):\n",
    "        self.p = p\n",
    "    def __call__(self, sample):\n",
    "        images, mask = sample['input'], sample['target_mask']\n",
    "        ct_img = images[:,:,:,0]\n",
    "        pet_img = images[:,:,:,1]\n",
    "        if random.random()<self.p:\n",
    "            new_ct, new_pet, new_mask = elasticdeform.deform_random_grid([ct_img,pet_img,mask],sigma = random.randint(5, 10), points =  random.randint(1,3),axis=(0, 1, 2))\n",
    "            new_mask = (new_mask - np.min(new_mask))/(np.max(new_mask) - np.min(new_mask))\n",
    "            new_mask[new_mask<0.5] = 0\n",
    "            new_mask[new_mask>0] = 1\n",
    "            img = np.stack([new_ct, new_pet], axis=-1)\n",
    "            sample['input'], sample['target_mask'] =img,new_mask\n",
    "        return sample\n",
    "class ElasticDeformation():\n",
    "    def __init__(self, p = 0.5):\n",
    "        self.p = p\n",
    "    def __call__(self, sample):\n",
    "        images, mask = sample['input'], sample['target_mask']\n",
    "        ct_img = images[:,:,:,0]\n",
    "        pet_img = images[:,:,:,1]\n",
    "        if random.random()<self.p:\n",
    "            new_ct, new_pet, new_mask = elasticdeform.deform_random_grid([ct_img,pet_img,mask],sigma = random.randint(5, 10), points =  random.randint(1,3),axis=(0, 1, 2))\n",
    "            new_mask = (new_mask - np.min(new_mask))/(np.max(new_mask) - np.min(new_mask))\n",
    "            new_mask[new_mask<0.5] = 0\n",
    "            new_mask[new_mask>0] = 1\n",
    "            img = np.stack([new_ct, new_pet], axis=-1)\n",
    "            sample['input'], sample['target_mask'] =img,new_mask\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_centroid(mask: sitk.Image):\n",
    "\n",
    "    stats = sitk.LabelShapeStatisticsImageFilter()\n",
    "    stats.Execute(mask)\n",
    "    centroid_coords = stats.GetCentroid(1)\n",
    "    centroid_idx = mask.TransformPhysicalPointToIndex(centroid_coords)\n",
    "\n",
    "    return np.asarray(centroid_idx, dtype=np.float64)\n",
    "\n",
    "def get_paths_to_patient_files(path_to_imgs, PatientID, append_mask=True):\n",
    "        \"\"\"\n",
    "    Get paths to all data samples, i.e., CT & PET images (and a mask) for each patient.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path_to_imgs : str\n",
    "        A path to a directory with patients' data. Each folder in the directory must corresponds to a single patient.\n",
    "    append_mask : bool\n",
    "        Used to append a path to a ground truth mask.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of tuple\n",
    "        A list wherein each element is a tuple with two (three) `pathlib.Path` objects for a single patient.\n",
    "        The first one is the path to the CT image, the second one - to the PET image. If `append_mask` is True,\n",
    "        the path to the ground truth mask is added.\n",
    "    \"\"\"\n",
    "        path_to_imgs = pathlib.Path(path_to_imgs)\n",
    "\n",
    "        patients = [p for p in PatientID if os.path.isdir(path_to_imgs / p)]\n",
    "        paths = []\n",
    "        for p in patients:\n",
    "            path_to_ct = path_to_imgs / p / (p + '_ct.nii.gz')\n",
    "            path_to_pt = path_to_imgs / p / (p + '_pt.nii.gz')\n",
    "\n",
    "            if append_mask:\n",
    "                path_to_mask = path_to_imgs / p / (p + '_ct_gtvt.nii.gz')\n",
    "                paths.append((path_to_ct, path_to_pt, path_to_mask))\n",
    "            else:\n",
    "                paths.append((path_to_ct, path_to_pt))\n",
    "        return paths\n",
    "\n",
    "class HecktorDataset(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 root_directory:str, \n",
    "                 clinical_data_path:str, \n",
    "                 patch_size:int =50,\n",
    "                 time_bins:int = 14,\n",
    "                 cache_dir:str = \"data_cropped/data_cache/\",\n",
    "                 transform: Optional[Callable] = None,\n",
    "                 num_workers: int = 1\n",
    "    ):\n",
    "\n",
    "        self.num_of_seqs = 2 #CT PT\n",
    "        \n",
    "        self.root_directory = root_directory\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.transforms = transform\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        self.clinical_data = self.make_data(clinical_data_path)\n",
    "        self.time_bins = make_time_bins(times=self.clinical_data[\"time\"], num_bins=time_bins, event = self.clinical_data[\"event\"])\n",
    "        self.y = encode_survival(self.clinical_data[\"time\"].values, self.clinical_data[\"event\"].values, self.time_bins) # single event\n",
    "\n",
    "        self.cache_path = get_paths_to_patient_files(cache_dir, self.clinical_data['PatientID'])\n",
    "\n",
    "\n",
    "    def make_data(self, path):\n",
    "\n",
    "        try:\n",
    "            X = pd.read_csv(path + '/hecktor2021_patient_info_training.csv')\n",
    "            y = pd.read_csv(path + '/hecktor2021_patient_endpoint_training.csv')\n",
    "            df = pd.merge(X, y, on=\"PatientID\")\n",
    "        except:\n",
    "            df = path\n",
    "\n",
    "        clinical_data = df\n",
    "        clinical_data = clinical_data.rename(columns={\"Progression\": \"event\", \"Progression free survival\": \"time\", \"TNM group\":\"Stage_group\", \"Gender (1=M,0=F)\":\"Gender\"})\n",
    "\n",
    "        clinical_data[\"Age\"] = scale(clinical_data[\"Age\"])\n",
    "\n",
    "        # binarize T stage as T1/2 = 0, T3/4 = 1\n",
    "        clinical_data[\"T-stage\"] = clinical_data[\"T-stage\"].map(\n",
    "            lambda x: \"T1/2\" if x in [\"T1\", \"T2\"] else(\"Tx\" if x == \"Tx\" else \"T3/4\"), na_action=\"ignore\")\n",
    "\n",
    "        # use more fine-grained grouping for N stage\n",
    "        clinical_data[\"N-stage\"] = clinical_data[\"N-stage\"].str.slice(0, 2)\n",
    "\n",
    "        clinical_data[\"Stage_group\"] = clinical_data[\"Stage_group\"].map(\n",
    "            lambda x: \"I/II\" if x in [\"I\", \"II\"] else \"III/IV\", na_action=\"ignore\")\n",
    "\n",
    "        clinical_data = pd.get_dummies(clinical_data,\n",
    "                                    columns=[\"Gender\",\n",
    "                                                \"N-stage\",\n",
    "                                                \"M-stage\",],\n",
    "                                    drop_first=True)\n",
    "\n",
    "        cols_to_drop = [\n",
    "            #\"PatientID\",\n",
    "            \"Tobacco\",\n",
    "            \"Alcohol\",\n",
    "            \"Performance status\",\n",
    "            \"HPV status (0=-, 1=+)\",\n",
    "            \"Estimated weight (kg) for SUV\",\n",
    "            \"CenterID\",\n",
    "\n",
    "        ]\n",
    "\n",
    "        clinical_data = clinical_data.drop(cols_to_drop, axis=1)\n",
    "\n",
    "\n",
    "        clinical_data = pd.get_dummies(clinical_data,\n",
    "                                    columns=[\"T-stage\",\n",
    "                                                \"Stage_group\",])\n",
    "        \n",
    "        return clinical_data\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def _prepare_data(self):\n",
    "        \"\"\"Preprocess and cache the dataset\"\"\"\n",
    "\n",
    "        Parallel(n_jobs=self.num_workers)(\n",
    "            delayed(self._preprocess_subject)(subject_id)\n",
    "            for subject_id in self.clinical_data[\"PatientID\"]\n",
    "        )\n",
    "\n",
    "    def _preprocess_subject(self, subject_id: str):\n",
    "\n",
    "        print(self.root_directory)\n",
    "        print(subject_id)\n",
    "        \n",
    "        path = os.path.join(self.root_directory, \"data/hecktor_nii/\"\n",
    "                            \"{}\",f\"{subject_id}\"+\"{}\"+\".nii\")\n",
    "\n",
    "        image = sitk.ReadImage(path.format(\"images\", \"_ct\"))\n",
    "        mask = sitk.ReadImage(path.format(\"masks\", \"_gtvt\"))\n",
    "\n",
    "        #crop the image to (patch_size)^3 patch around the tumor center\n",
    "        tumour_center = find_centroid(mask)\n",
    "        size = np.ceil(self.patch_size / np.asarray(image.GetSpacing())).astype(np.int) + 1\n",
    "        min_coords = np.floor(tumour_center - size / 2).astype(np.int64)\n",
    "        max_coords = np.floor(tumour_center + size / 2).astype(np.int64)\n",
    "        min_x, min_y, min_z = min_coords\n",
    "        max_x, max_y, max_z = max_coords\n",
    "        image = image[min_x:max_x, min_y:max_y, min_z:max_z]\n",
    "\n",
    "        # resample to isotropic 1 mm spacing\n",
    "        reference_image = sitk.Image([self.patch_size]*3, sitk.sitkFloat32)\n",
    "        reference_image.SetOrigin(image.GetOrigin())\n",
    "        image = sitk.Resample(image, reference_image)\n",
    "\n",
    "        # window image intensities to [-500, 1000] HU range\n",
    "        image = sitk.Clamp(image, sitk.sitkFloat32, -500, 1000)\n",
    "\n",
    "        sitk.WriteImage(image, os.path.join(self.cache_path, f\"{subject_id}.nii\"), True)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
    "        \"\"\"Get an input-target pair from the dataset.\n",
    "\n",
    "        The images are assumed to be preprocessed and cached.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx\n",
    "            The index to retrieve (note: this is not the subject ID).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple of torch.Tensor and int\n",
    "            The input-target pair.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:      # training data\n",
    "            # clin_var_data = self.clinical_data.drop([\"target_binary\", 'time', 'event', 'Study ID'], axis=1) # single event\n",
    "            clin_var_data = self.clinical_data.drop(['PatientID','time', 'event'], axis=1)\n",
    "        except:   # test data\n",
    "            clin_var_data = self.clinical_data.drop(['PatientID'], axis=1)\n",
    "\n",
    "\n",
    "        clin_var = clin_var_data.iloc[idx].to_numpy(dtype='float32')\n",
    "        \n",
    "        target = self.y[idx]\n",
    "        \n",
    "        labels = self.clinical_data.iloc[idx].to_dict()\n",
    " \n",
    "        \n",
    "        subject_id = self.clinical_data.iloc[idx][\"PatientID\"]\n",
    "        # path = self.cache_path, f\"{subject_id}_ct.nii.gz\")\n",
    "#         print('hi:', path)\n",
    "        \n",
    "        # image = sitk.ReadImage(path)\n",
    "        # if self.transform is not None:\n",
    "        #     image = self.transform(image)\n",
    "        \n",
    "        \n",
    "        sample = dict()\n",
    "        \n",
    "        id_ = self.cache_path[idx][0].parent.stem\n",
    "\n",
    "        sample['id'] = id_\n",
    "        img = [self.read_data(self.cache_path[idx][i]) for i in range(self.num_of_seqs)]\n",
    "        img = np.stack(img, axis=-1)\n",
    "        #img = rearrange(img,'h w d c -> c h w d')\n",
    "        sample['input'] = img #np.expand_dims(img, axis=0)\n",
    "        \n",
    "        mask = self.read_data(self.cache_path[idx][-1])\n",
    "        mask = np.expand_dims(mask, axis=3)\n",
    "        #mask = rearrange(mask,'h w d c->c h w d')\n",
    "        sample['target_mask'] = mask\n",
    "        \n",
    "        if self.transforms:\n",
    "            sample = self.transforms(sample)\n",
    "    \n",
    "        return (sample, clin_var), target, labels\n",
    "    \n",
    "    \n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return the length of the dataset.\"\"\"\n",
    "        return len(self.clinical_data)\n",
    "    \n",
    "    @staticmethod\n",
    "    def read_data(path_to_nifti, return_numpy=True):\n",
    "        \"\"\"Read a NIfTI image. Return a numpy array (default) or `nibabel.nifti1.Nifti1Image` object\"\"\"\n",
    "        if return_numpy:\n",
    "            return nib.load(str(path_to_nifti)).get_fdata()\n",
    "        return nib.load(str(path_to_nifti))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clin_var = 12\n",
    "\n",
    "\n",
    "def flatten_layers(arr):\n",
    "    return [i for sub in arr for i in sub]\n",
    "\n",
    "\n",
    "class UNETR(nn.Module):\n",
    "    \"\"\"\n",
    "    UNETR based on: \"Hatamizadeh et al.,\n",
    "    UNETR: Transformers for 3D Medical Image Segmentation <https://arxiv.org/abs/2103.10504>\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "                    self,\n",
    "                    hparams : dict,\n",
    "                    in_channels: int,\n",
    "                    out_channels: int,\n",
    "                    img_size: Union[Sequence[int], int],\n",
    "                    feature_size: int = 16,\n",
    "                    hidden_size: int = 768,\n",
    "                    mlp_dim: int = 3072,\n",
    "                    num_heads: int = 12,\n",
    "                    pos_embed: str = \"conv\",\n",
    "                    norm_name: Union[Tuple, str] = \"instance\",\n",
    "                    conv_block: bool = True,\n",
    "                    res_block: bool = True,\n",
    "                    dropout_rate: float = 0.0,\n",
    "                    spatial_dims: int = 3,\n",
    "                ) -> None:\n",
    "\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        if not (0 <= dropout_rate <= 1):\n",
    "            raise ValueError(\"dropout_rate should be between 0 and 1.\")\n",
    "\n",
    "        if hidden_size % num_heads != 0:\n",
    "            raise ValueError(\"hidden_size should be divisible by num_heads.\")\n",
    "\n",
    "        self.num_layers = 12\n",
    "        img_size = ensure_tuple_rep(img_size, spatial_dims)\n",
    "        self.patch_size = ensure_tuple_rep(16, spatial_dims)\n",
    "        self.feat_size = tuple(img_d // p_d for img_d, p_d in zip(img_size, self.patch_size))\n",
    "        self.hidden_size = hidden_size\n",
    "        self.classification = False\n",
    "        self.vit = ViT(\n",
    "            in_channels=in_channels,\n",
    "            img_size=img_size,\n",
    "            patch_size=self.patch_size,\n",
    "            hidden_size=hidden_size,\n",
    "            mlp_dim=mlp_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            num_heads=num_heads,\n",
    "            pos_embed=pos_embed,\n",
    "            classification=self.classification,\n",
    "            dropout_rate=dropout_rate,\n",
    "            spatial_dims=spatial_dims,\n",
    "        )\n",
    "        \n",
    "        self.encoder1 = UnetrBasicBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=in_channels,\n",
    "            out_channels=feature_size,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.encoder2 = UnetrPrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=hidden_size,\n",
    "            out_channels=feature_size * 2,\n",
    "            num_layer=2,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            conv_block=conv_block,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.encoder3 = UnetrPrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=hidden_size,\n",
    "            out_channels=feature_size * 4,\n",
    "            num_layer=1,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            conv_block=conv_block,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.encoder4 = UnetrPrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=hidden_size,\n",
    "            out_channels=feature_size * 8,\n",
    "            num_layer=0,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            conv_block=conv_block,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.decoder5 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=hidden_size,\n",
    "            out_channels=feature_size * 8,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.decoder4 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=feature_size * 8,\n",
    "            out_channels=feature_size * 4,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.decoder3 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=feature_size * 4,\n",
    "            out_channels=feature_size * 2,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.decoder2 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=feature_size * 2,\n",
    "            out_channels=feature_size,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.out = UnetOutBlock(spatial_dims=spatial_dims, in_channels=feature_size, out_channels=out_channels)\n",
    "        \n",
    "        config = {}\n",
    "        config['num_of_attention_heads'] = 12\n",
    "        config['hidden_size'] = 768\n",
    "        # self.msa = BertSelfAttention(config)\n",
    "\n",
    "        if hparams['n_dense'] <=0:\n",
    "            self.mtlr = MTLR(hparams['hidden_size'] , hparams['time_bins'])\n",
    "\n",
    "        else:\n",
    "            fc_layers = [[nn.Linear(hparams['hidden_size'] , 256 * hparams['dense_factor']), \n",
    "                          nn.BatchNorm1d(256 * hparams['dense_factor']),\n",
    "                          nn.ReLU(inplace=True), \n",
    "                          nn.Dropout(hparams['dropout'])]]   \n",
    "            \n",
    "            if hparams['n_dense'] > 1:    \n",
    "                fc_layers.extend([[nn.Linear(256 * hparams['dense_factor'], 64 * hparams['dense_factor']),\n",
    "                                   nn.BatchNorm1d(64 * hparams['dense_factor']),\n",
    "                                   nn.ReLU(inplace=True),\n",
    "                                   nn.Dropout(hparams['dropout'])] for _ in range(hparams['n_dense'] - 1)])\n",
    "            \n",
    "            fc_layers = flatten_layers(fc_layers)\n",
    "            self.mtlr = nn.Sequential(*fc_layers,\n",
    "                                      MTLR(64 * hparams['dense_factor'], hparams['time_bins']),)\n",
    "           \n",
    "            # self.dimred = nn.Linear(768, 256)\n",
    "            # self.ehrproj = nn.Linear(12, 256)\n",
    "                            \n",
    "            # self.mtlr = nn.Sequential(\n",
    "            #                           nn.Linear(768, 512),\n",
    "            #                           nn.BatchNorm1d(512),\n",
    "            #                           nn.ReLU(inplace=True), \n",
    "            #                           nn.Dropout(hparams['dropout']),\n",
    "            #                           nn.Linear(512, 128),\n",
    "            #                           nn.BatchNorm1d(128),\n",
    "            #                           nn.ReLU(inplace=True), \n",
    "            #                           nn.Dropout(hparams['dropout']),\n",
    "            #                           MTLR(128, hparams['time_bins']))\n",
    "\n",
    "    def proj_feat(self, x, hidden_size, feat_size):\n",
    "        new_view = (x.size(0), *feat_size, hidden_size)\n",
    "        x = x.view(new_view)\n",
    "        new_axes = (0, len(x.shape) - 1) + tuple(d + 1 for d in range(len(feat_size)))\n",
    "        x = x.permute(new_axes).contiguous()\n",
    "        return x\n",
    "\n",
    "    def forward(self, sample):\n",
    "\n",
    "        sample_img, clin_var = sample\n",
    "        x_in = (sample_img['input'], clin_var)\n",
    "        \n",
    "        x, hidden_states_out = self.vit(x_in)\n",
    "\n",
    "        \n",
    "        enc1 = self.encoder1(sample_img['input'])\n",
    "        x2 = hidden_states_out[3][:,1:,:]\n",
    "        enc2 = self.encoder2(self.proj_feat(x2, self.hidden_size, self.feat_size))\n",
    "        x3 = hidden_states_out[6][:,1:,:]\n",
    "        enc3 = self.encoder3(self.proj_feat(x3, self.hidden_size, self.feat_size))\n",
    "        x4 = hidden_states_out[9][:,1:,:]\n",
    "        enc4 = self.encoder4(self.proj_feat(x4, self.hidden_size, self.feat_size))\n",
    "        dec4 = self.proj_feat(x[:,1:,:], self.hidden_size, self.feat_size)\n",
    "        dec3 = self.decoder5(dec4, enc4)\n",
    "        dec2 = self.decoder4(dec3, enc3)\n",
    "        dec1 = self.decoder3(dec2, enc2)\n",
    "        out = self.decoder2(dec1, enc1)\n",
    "        \n",
    "        x = torch.mean(x, dim=1)\n",
    "        # x = x[:,:,:n_clin_var]\n",
    "        # x = self.dimred(x)\n",
    "        # clin_var = self.ehrproj(clin_var)\n",
    "        # f_con = x + clin_var\n",
    "        # f_con = torch.cat((clin_var, x), dim=1)        \n",
    "\n",
    "        # x_mtlr = torch.mean(f_con, dim=1)\n",
    "        # msa = self.msa(f_con)\n",
    "\n",
    "        risk_out = self.mtlr(x)\n",
    "        \n",
    "        return self.out(out), risk_out \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Copyright 2020 - 2021 MONAI Consortium\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\n",
    "from typing import Sequence, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from monai.networks.blocks.patchembedding import PatchEmbeddingBlock\n",
    "from monai.networks.blocks.transformerblock import TransformerBlock\n",
    "\n",
    "# __all__ = [\"ViT\"]\n",
    "\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision Transformer (ViT), based on: \"Dosovitskiy et al.,\n",
    "    An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale <https://arxiv.org/abs/2010.11929>\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        img_size: Union[Sequence[int], int],\n",
    "        patch_size: Union[Sequence[int], int],\n",
    "        hidden_size: int = 768,\n",
    "        mlp_dim: int = 3072,\n",
    "        num_layers: int = 12,\n",
    "        num_heads: int = 12,\n",
    "        pos_embed: str = \"conv\",\n",
    "        classification: bool = False,\n",
    "        num_classes: int = 2,\n",
    "        dropout_rate: float = 0.0,\n",
    "        spatial_dims: int = 3,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels: dimension of input channels.\n",
    "            img_size: dimension of input image.\n",
    "            patch_size: dimension of patch size.\n",
    "            hidden_size: dimension of hidden layer.\n",
    "            mlp_dim: dimension of feedforward layer.\n",
    "            num_layers: number of transformer blocks.\n",
    "            num_heads: number of attention heads.\n",
    "            pos_embed: position embedding layer type.\n",
    "            classification: bool argument to determine if classification is used.\n",
    "            num_classes: number of classes if classification is used.\n",
    "            dropout_rate: faction of the input units to drop.\n",
    "            spatial_dims: number of spatial dimensions.\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            # for single channel input with image size of (96,96,96), conv position embedding and segmentation backbone\n",
    "            >>> net = ViT(in_channels=1, img_size=(96,96,96), pos_embed='conv')\n",
    "\n",
    "            # for 3-channel with image size of (128,128,128), 24 layers and classification backbone\n",
    "            >>> net = ViT(in_channels=3, img_size=(128,128,128), pos_embed='conv', classification=True)\n",
    "\n",
    "            # for 3-channel with image size of (224,224), 12 layers and classification backbone\n",
    "            >>> net = ViT(in_channels=3, img_size=(224,224), pos_embed='conv', classification=True, spatial_dims=2)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        if not (0 <= dropout_rate <= 1):\n",
    "            raise ValueError(\"dropout_rate should be between 0 and 1.\")\n",
    "\n",
    "        if hidden_size % num_heads != 0:\n",
    "            raise ValueError(\"hidden_size should be divisible by num_heads.\")\n",
    "\n",
    "        self.classification = classification\n",
    "        self.patch_embedding = PatchEmbeddingBlock(\n",
    "            in_channels=in_channels,\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_heads=num_heads,\n",
    "            pos_embed=pos_embed,\n",
    "            dropout_rate=dropout_rate,\n",
    "            spatial_dims=spatial_dims,\n",
    "        )\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [TransformerBlock(hidden_size, mlp_dim, num_heads, dropout_rate) for i in range(num_layers)]\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        if self.classification:\n",
    "            self.cls_token = nn.Parameter(torch.zeros(1, 1, hidden_size))\n",
    "            self.classification_head = nn.Sequential(nn.Linear(hidden_size, num_classes), nn.Tanh())\n",
    "            \n",
    "        ## Projection of EHR\n",
    "        self.EHR_proj = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        ## Position embedding for EHR\n",
    "        #self.EHR_pos = nn.Parameter(torch.zeros(1, 1, hidden_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.patch_embedding(x) #img, clin_var = x\n",
    "\n",
    "        #clin_var = self.EHR_proj(clin_var)\n",
    "        #clin_var = clin_var.repeat(1,64)\n",
    "        #clin_var = self.EHR_proj(clin_var)\n",
    "        #clin_var = clin_var.unsqueeze(1)\n",
    "        #clin_var = clin_var + self.EHR_pos\n",
    "        \n",
    "        #x = torch.cat((clin_var, x), dim=1)\n",
    "        \n",
    "        if self.classification:\n",
    "            cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "            x = torch.cat((cls_token, x), dim=1)\n",
    "        hidden_states_out = []\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "            hidden_states_out.append(x)\n",
    "        x = self.norm(x)\n",
    "        if self.classification:\n",
    "            x = self.classification_head(x[:, 0])\n",
    "            \n",
    "        return x, hidden_states_out\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Copyright 2020 - 2021 MONAI Consortium\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\n",
    "import math\n",
    "from typing import Sequence, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from monai.networks.layers import Conv\n",
    "from monai.utils import ensure_tuple_rep, optional_import\n",
    "from monai.utils.module import look_up_option\n",
    "\n",
    "Rearrange, _ = optional_import(\"einops.layers.torch\", name=\"Rearrange\")\n",
    "SUPPORTED_EMBEDDING_TYPES = {\"conv\", \"perceptron\"}\n",
    "\n",
    "\n",
    "class PatchEmbeddingBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A patch embedding block, based on: \"Dosovitskiy et al.,\n",
    "    An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale <https://arxiv.org/abs/2010.11929>\"\n",
    "\n",
    "    Example::\n",
    "\n",
    "        >>> from monai.networks.blocks import PatchEmbeddingBlock\n",
    "        >>> PatchEmbeddingBlock(in_channels=4, img_size=32, patch_size=8, hidden_size=32, num_heads=4, pos_embed=\"conv\")\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        img_size: Union[Sequence[int], int],\n",
    "        patch_size: Union[Sequence[int], int],\n",
    "        hidden_size: int,\n",
    "        num_heads: int,\n",
    "        pos_embed: str,\n",
    "        dropout_rate: float = 0.0,\n",
    "        spatial_dims: int = 3,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels: dimension of input channels.\n",
    "            img_size: dimension of input image.\n",
    "            patch_size: dimension of patch size.\n",
    "            hidden_size: dimension of hidden layer.\n",
    "            num_heads: number of attention heads.\n",
    "            pos_embed: position embedding layer type.\n",
    "            dropout_rate: faction of the input units to drop.\n",
    "            spatial_dims: number of spatial dimensions.\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        if not (0 <= dropout_rate <= 1):\n",
    "            raise ValueError(\"dropout_rate should be between 0 and 1.\")\n",
    "\n",
    "        if hidden_size % num_heads != 0:\n",
    "            raise ValueError(\"hidden size should be divisible by num_heads.\")\n",
    "\n",
    "        self.pos_embed = look_up_option(pos_embed, SUPPORTED_EMBEDDING_TYPES)\n",
    "\n",
    "        img_size = ensure_tuple_rep(img_size, spatial_dims)\n",
    "        patch_size = ensure_tuple_rep(patch_size, spatial_dims)\n",
    "        for m, p in zip(img_size, patch_size):\n",
    "            if m < p:\n",
    "                raise ValueError(\"patch_size should be smaller than img_size.\")\n",
    "            if self.pos_embed == \"perceptron\" and m % p != 0:\n",
    "                raise ValueError(\"patch_size should be divisible by img_size for perceptron.\")\n",
    "        self.n_patches = np.prod([im_d // p_d for im_d, p_d in zip(img_size, patch_size)]) + 1  # +1 for EHR\n",
    "        self.patch_dim = in_channels * np.prod(patch_size)\n",
    "\n",
    "        self.patch_embeddings: nn.Module\n",
    "        if self.pos_embed == \"conv\":\n",
    "            self.patch_embeddings = Conv[Conv.CONV, spatial_dims](\n",
    "                in_channels=in_channels, out_channels=hidden_size, kernel_size=patch_size, stride=patch_size\n",
    "            )\n",
    "        elif self.pos_embed == \"perceptron\":\n",
    "            # for 3d: \"b c (h p1) (w p2) (d p3)-> b (h w d) (p1 p2 p3 c)\"\n",
    "            chars = ((\"h\", \"p1\"), (\"w\", \"p2\"), (\"d\", \"p3\"))[:spatial_dims]\n",
    "            from_chars = \"b c \" + \" \".join(f\"({k} {v})\" for k, v in chars)\n",
    "            to_chars = f\"b ({' '.join([c[0] for c in chars])}) ({' '.join([c[1] for c in chars])} c)\"\n",
    "            axes_len = {f\"p{i+1}\": p for i, p in enumerate(patch_size)}\n",
    "            self.patch_embeddings = nn.Sequential(\n",
    "                Rearrange(f\"{from_chars} -> {to_chars}\", **axes_len), nn.Linear(self.patch_dim, hidden_size)\n",
    "            )\n",
    "            \n",
    "        self.EHR_proj = nn.Sequential(nn.Linear(n_clin_var, hidden_size),\n",
    "                                    #   nn.ReLU(inplace=True)\n",
    "                                      )\n",
    "        # self.n_patches = 76\n",
    "\n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, self.n_patches, hidden_size))\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, hidden_size))\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.trunc_normal_(self.position_embeddings, mean=0.0, std=0.02, a=-2.0, b=2.0)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            self.trunc_normal_(m.weight, mean=0.0, std=0.02, a=-2.0, b=2.0)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def trunc_normal_(self, tensor, mean, std, a, b):\n",
    "        # From PyTorch official master until it's in a few official releases - RW\n",
    "        # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
    "        def norm_cdf(x):\n",
    "            return (1.0 + math.erf(x / math.sqrt(2.0))) / 2.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            l = norm_cdf((a - mean) / std)\n",
    "            u = norm_cdf((b - mean) / std)\n",
    "            tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
    "            tensor.erfinv_()\n",
    "            tensor.mul_(std * math.sqrt(2.0))\n",
    "            tensor.add_(mean)\n",
    "            tensor.clamp_(min=a, max=b)\n",
    "            return tensor\n",
    "\n",
    "    def forward(self, x):\n",
    "        img, clin_var = x\n",
    "        x = self.patch_embeddings(img)\n",
    "        \n",
    "        clin_var = self.EHR_proj(clin_var)\n",
    "        clin_var = clin_var.unsqueeze(dim=1)\n",
    "        # clin_var = repeat(clin_var, 'b c d->b (repeat c) d', repeat=75)\n",
    "\n",
    "        if self.pos_embed == \"conv\":\n",
    "            x = x.flatten(2).transpose(-1, -2)\n",
    "            \n",
    "        x = torch.cat([clin_var, x], dim=1)\n",
    "\n",
    "\n",
    "\n",
    "        embeddings = x + self.position_embeddings\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set =  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
